{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:08a146392b1207883ba557d0846060f449f31fcd9bbdb0ddd0d74ebcc78ae1f6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 id=\"tocheading\" align=\"center\" style=\"font-size:32px\">Introduction to Python</h1>\n",
      "<div align=\"center\" style=\"font-size:24px\">15.S60 Software Tools for Operations Research</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<h1 id=\"tocheading\">Table of Contents</h1>\n",
      "<div id=\"toc\"></div>\n",
      "<script src=\"https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js\"></script>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Introduction\n",
      "Why use Python?\n",
      "\n",
      "- Free, widely-used language for data analysis\n",
      "- Many packages available for statistics, machine learning\n",
      "- Can be used for scripting, data manipulation and analysis, and visualization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##15.S60 (2014)\n",
      "\n",
      "Today\n",
      "\n",
      "- IPython environment\n",
      "- Scripting, basic data manipulation\n",
      "- Packages: NumPy, Pandas, Matplotlib, Statsmodels, Scikit-learn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "##Resources and References\n",
      "\n",
      "- Official website: http://ipython.org/\n",
      "    - Usually bundled together in *distributions* such as [Anaconda](https://store.continuum.io/cshop/anaconda/), [Enthought Canopy](https://www.enthought.com/products/canopy/), [PythonXY](https://code.google.com/p/pythonxy/), etc\n",
      "    - Current version: \n",
      "    - lots of [extensions available](https://github.com/ipython-contrib/IPython-notebook-extensions/wiki)\n",
      "\n",
      "\n",
      "- Other helpful sites"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Basics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Linear and Logistic Regression\n",
      "- used for binary classification (e.g. predict predict whether or not a passenger survived)\n",
      "- Uses a logistic function to predict the probability of a class\n",
      "\n",
      "    $\\mathbb{P}(\\hat{y}=1)=\\frac{1}{1+e^{-(b_0+b_1x_1+\\dots+b_kx_k)}}$\n",
      "    \n",
      "- Think about the term in the exponent as a linear regression; the logistic function then translates the estimate from the linear regression to a value between 0 and 1\n",
      "- We then use a threshold value on the output of the logistic function\n",
      "    - Often, this value is 0.5, meaning that we predict the class with the higher probability\n",
      "    - Sometimes other threshold values may be more appropriate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "#Classification and Regression Trees (CART)\n",
      "- Make sequential splits on independent variables (was the passenger male? if yes, then consider age; if no, then consider class, and so on)\n",
      "- Splits are made to make the \u201cbuckets\u201d as \u201cpure\u201d as possible, in a greedy fashion\n",
      "\n",
      "![](./img/cart1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "- You can usually build a tree that fits the data exactly (except when two identical data points have differing classifications)\n",
      "- Thus, we have to limit the power of the algorithm so that we don\u2019t overfit (strong performance on training data, but poor ability to generalize to test data and beyond)\n",
      "    - `minbucket` requires each terminal leaf to have at least some number of points\n",
      "    - `minsplit` will require a minimum number of points in a bucket before a split can be made\n",
      "    - Can also limit the number of splits\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "When might CART perform poorly?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "- Predictions made by CART trees can be multi-class categorical (e.g., yes or no; red, green, or blue, etc.). In this case, the prediction for a test data point is just the label of the majority of the points in that \u201cbucket\u201d\n",
      "- Predictions can also be continuous (e.g., price, distance, etc.). In this case, the prediction for a test data point is just the average of the points in that \u201cbucket\u201d"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Random Forest\n",
      "- The random forest algorithm builds many CART trees that are uncorrelated\n",
      "- For each CART tree, select only a subset of the data points on which to regress\n",
      "- When making splits, only choose a subset of attributes that are permitted to be split upon (changes at each iteration of splits)\n",
      "    - Defaults in R (which can be changed using `mtry`) are $\\sqrt m$ for classification and $m/3$ for regression\n",
      "- Prediction is the majority \u201cvote\u201d for classification and the mean of all trees for continuous outcomes\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "##Types of Clustering\n",
      "- Hierarchical (Agglomerative) Clustering\n",
      "    - Each data point starts out as its own cluster. Then merge based on a given criterion until there is only one cluster remaining.\n",
      "    - No need to specify the number of clusters, since you can visualize the hierarchy.\n",
      "- K-means Clustering\n",
      "    - Begin by specifying the centroids of k clusters. Assign each data point to the nearest cluster. Then re-center the resultant clusters and iterate.\n",
      "    - Less computationally intensive than hierarchical clustering, so better for larger data sets.\n",
      "- Many other types of clustering as well"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "##Distance Metrics\n",
      "- In order for us to cluster data points, we need to define a notion of distance between data points.\n",
      "    - Today, we will use Euclidean distance ($L_2$ norm)\n",
      "    - Manhattan ($L_1$ norm) and Maximum coordinate ($L_{inf}$ norm) are common as well\n",
      "    - Often is highly sensitive to scale (we can normalize by subtracting the mean and dividing by the standard deviation)\n",
      "- We also need to define distances between two clusters.\n",
      "    - Today, we will use centroid distance\n",
      "    - Maximum and minimum cluster distances can be used as well"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "##Hierarchical Clustering versus K-Means\n",
      "- Advantages of Hierarchical Clustering:\n",
      "    - No need to specify number of clusters in advance (use dendrograms to visualize)\n",
      "    - Clusters are nested (you can see the hierarchy)\n",
      "- Advantages of K-means Clustering:\n",
      "    - Faster (important for large data sets)\n",
      "    - Less influenced by choice of distance metric\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Support Vector Machines\n",
      "- At a high level, SVMs determine a decision boundary that maximizes the \u201cmargin\u201d (which, roughly, is the distance from the boundary to the support vectors)\n",
      "- The objective is to maximize this margin, while the constraints are to obey the labels on the points\n",
      "- A nonlinear optimization problem, but can be solved efficiently (if interested, see Wikipedia article on SVMs)\n",
      "- Using kernels, decision boundaries can be extended to polynomials and beyond (Gaussian, or radial basis, kernels can be constructed so that any training set can be classified correctly)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#Conclusion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<h1 id=\"tocheading\" style=\"font-size:28px\">Thanks for listening!</h1>\n",
      "- Special thanks to Allison O\u2019Hair\n",
      "- Solutions can be found at Git: `https://github.com/IainNZ/ORSoftwareTools2014`\n",
      "- Please fill out feedback forms!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}